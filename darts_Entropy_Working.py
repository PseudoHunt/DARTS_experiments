# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ty_SfaztOa6QJ2O4YJvsS8SKZowLoRL
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import CIFAR10
from torchvision.transforms import transforms
from tqdm import tqdm
import random
import numpy as np
import torch.nn.functional as F

class SepConv(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
        super(SepConv, self).__init__()

        self.op = nn.Sequential(
            nn.Conv2d(C_in, C_in, kernel_size, stride, padding, groups=C_in, bias=False),
            nn.BatchNorm2d(C_in, affine=affine),
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, 1, 1, 0, bias=False),
            nn.BatchNorm2d(C_in, affine=affine),
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size, stride, padding, groups=C_in, bias=False),
            nn.BatchNorm2d(C_in, affine=affine),
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, 1, 1, 0, bias=False),
            nn.BatchNorm2d(C_out, affine=affine),
            nn.ReLU(inplace=False),
        )

    def forward(self, x):
        return self.op(x)


class DilConv(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):
        super(DilConv, self).__init__()

        self.op = nn.Sequential(
            nn.Conv2d(C_in, C_in, kernel_size, stride, padding, dilation=dilation, groups=C_in, bias=False),
            nn.BatchNorm2d(C_in, affine=affine),
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, 1, 1, 0, bias=False),
            nn.BatchNorm2d(C_out, affine=affine),
            nn.ReLU(inplace=False),
        )

    def forward(self, x):
        return self.op(x)

# Define the DARTS architecture search space
class MixedOp(nn.Module):
    def __init__(self, C, stride):
        super(MixedOp, self).__init__()
        self.ops = nn.ModuleList()
        for primitive in primitives:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self.ops.append(op)
        self.weights = nn.Parameter(torch.Tensor(len(primitives)).fill_(1))

    def forward(self, x):
        res = []
        for i, op in enumerate(self.ops):
            #w = nn.functional.gumbel_softmax(self.weights, tau=temperature, hard=True)
            #if self.training:
                # Straight-through estimator
                #hardw = nn.functional.one_hot(w.argmax(dim=-1), num_classes=len(primitives))
                #hardw = hardw.float()
                #self.weights.grad = (hardw - w).detach()
            res.append(op(x))
        return sum(w * op_res for w, op_res in zip(self.weights, res))

import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(in_planes, planes, kernel_size=5, stride=stride, padding=2, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.weights = nn.Parameter(torch.Tensor(2).fill_(1))

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion * planes)
            )

    def forward(self, x):
        branch_out = []
        out1 = F.relu(self.bn1(self.conv1(x)))
        out2 = self.bn2(self.conv2(x))
        branch_out.append(out1)
        branch_out.append(out2)
        #print(self.weights)
        out = self.weights[0].view(1, -1, 1, 1)*out1 + self.weights[1].view(1, -1, 1, 1)*out2
        #out = sum(w * op_res for w, op_res in zip(self.weights, branch_out))
        out = F.relu(out)
        return out



class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        # Layer 1
        self.layer1_0 = block(self.in_planes, 64, stride=1)
        self.layer1_1 = block(64, 64, stride=1)

        # Layer 2
        self.layer2_0 = block(64, 128, stride=2)
        self.layer2_1 = block(128, 128, stride=1)

        # Layer 3
        self.layer3_0 = block(128, 256, stride=2)
        self.layer3_1 = block(256, 256, stride=1)

        # Layer 4
        self.layer4_0 = block(256, 512, stride=2)
        self.layer4_1 = block(512, 512, stride=1)

        # Global average pooling and fully connected layer
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.linear = nn.Linear(512 * block.expansion, num_classes)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))

        # Forward pass through each residual block in the layers
        out = self.layer1_0(out)
        out = self.layer1_1(out)

        out = self.layer2_0(out)
        out = self.layer2_1(out)

        out = self.layer3_0(out)
        out = self.layer3_1(out)

        out = self.layer4_0(out)
        out = self.layer4_1(out)

        # Global average pooling
        out = self.avg_pool(out)
        out = out.view(out.size(0), -1)

        # Fully connected layer
        out = self.linear(out)

        return out

    def darts_weights(self):
        return [param for name, param in self.named_parameters() if 'weights' in name]

    def freeze_weights(self):
        for name, param in model.named_parameters():
          if 'weights' in name:
            param.requires_grad = False

    def unfreeze_weights(self):
        for name, param in model.named_parameters():
          if 'weights' in name:
            param.requires_grad = True

# Define the dataset and data loaders
transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=1)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=1)

# Set the random seed for reproducibility
random.seed(123)
np.random.seed(123)
torch.manual_seed(123)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(123)

# Set the hyperparameters
lr = 0.025
momentum = 0.9
weight_decay = 3e-4
epochs = 10
temperature = 0.1
primitives = ['sep_conv_3x3', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_5x5', 'max_pool_3x3', 'avg_pool_3x3']
num_classes = 10
layers = 8
steps = 4
multiplier = 1
stem_multiplier = 1
C = 16

def calculate_entropy(probabilities):
  # Ensure probabilities sum to 1 (normalization)
  probabilities /= torch.sum(probabilities)

  # Calculate binary entropy
  entropy = -torch.sum(probabilities * torch.log2(probabilities))
  return entropy

# Define the optimizer and loss function
#model = DARTS(C=C, num_classes=num_classes, layers=layers, steps=steps, multiplier=multiplier, stem_multiplier=stem_multiplier)
def ResNet18():
    return ResNet(BasicBlock, [2, 2, 2, 2])

# Create an instance of ResNet-18
model = ResNet18()
if torch.cuda.is_available():
    model.cuda()
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
darts_optimizer = optim.SGD(model.darts_weights(), lr=lr, momentum=momentum, weight_decay=weight_decay)
criterion = nn.CrossEntropyLoss()

# Train the DARTS model
for epoch in range(epochs):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    for inputs, targets in tqdm(train_loader):
        #train conv weights
        print("train 1")
        model.freeze_weights()
        if torch.cuda.is_available():
            inputs, targets = inputs.cuda(), targets.cuda()
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        print("loss")
        print(loss)
        print("train 2")

    for inputs, targets in tqdm(test_loader):
        #train darts weights
        model.unfreeze_weights()
        if torch.cuda.is_available():
            inputs, targets = inputs.cuda(), targets.cuda()
        darts_optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        #calculate entropy
        reg = 0
        for param in model.darts_weights():
          reg += calculate_entropy(param.data)
        print('loss,reg')
        print(loss,reg)
        loss = loss + 0.2 * reg
        loss.backward()
        darts_optimizer.step()
        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    print('Epoch:', epoch+1, 'Train Loss:', train_loss/len(train_loader), 'Train Accuracy:', 100.*correct/total)

# Discretize the architecture to obtain the final architecture
model.eval()
alphas = []
for name, param in model.named_parameters():
    if 'weights' in name:
        alpha = param.detach().cpu().numpy()
        alpha = np.argmax(alpha, axis=-1)
        alphas.append(alpha)
gene = np.concatenate([alpha.reshape(-1) for alpha in alphas])
#model_final = D

for name, param in model.named_parameters():
    if 'weights' in name:
        print(name, param.data)

