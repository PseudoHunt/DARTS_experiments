{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97y496sAYePw"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torchvision.models.mobilenetv2 import InvertedResidual\n",
        "import torch.nn as nn\n",
        "\n",
        "class BranchedBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BranchedBlock, self).__init__()\n",
        "        # MobileNetV2 block\n",
        "        self.mobilenet_block = InvertedResidual(in_channels, out_channels, stride, expand_ratio=6)\n",
        "        # Regular convolution block\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mobilenet_out = self.mobilenet_block(x)\n",
        "        conv_out = self.conv_block(x)\n",
        "        out = mobilenet_out + conv_out\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuPdBokDaE0J",
        "outputId": "1edb7ecc-b6bb-43cb-80c3-2e635d8d0e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.branch1 = BranchedBlock(3, 6)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.branch2 = BranchedBlock(6, 16)\n",
        "        self.fc1 = nn.Linear(16 * 8 * 8, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.branch1(x))\n",
        "        x = self.pool(self.branch2(x))\n",
        "        x = x.view(-1, 16 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_qhZuRdjY9YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training script remains the same\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Added std dev\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the network, loss function, and optimizer\n",
        "net = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # Get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:  # Print every 200 mini-batches\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save the trained model to ONNX format\n",
        "dummy_input = torch.randn(1, 3, 32, 32)\n",
        "torch.onnx.export(net, dummy_input, \"branched_cnn.onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F70Zx9JZArj",
        "outputId": "649df31c-9f55-4055-f377-a12f8b1d7c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[Epoch 1, Batch 200] loss: 2.156\n",
            "[Epoch 1, Batch 400] loss: 1.764\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxnnLux4atwG",
        "outputId": "715803ca-4a20-4c58-a4df-813108d9e4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-10.1.0.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt-cu12 (from tensorrt)\n",
            "  Downloading tensorrt-cu12-10.1.0.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorrt, tensorrt-cu12\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.1.0-py2.py3-none-any.whl size=16332 sha256=90e19b6e6c88d51e15cc3e8a87b225d4396688ffe5eb63e733b70d5b384c8dc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/55/f5/a1836546c0d92da062e9365a0323953f5e6a0a5f51d46da503\n",
            "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.1.0-py2.py3-none-any.whl size=17554 sha256=5ef0568929bdfa65d993b161832910cdc82cc0509162a61ade49fbe6925866bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/96/43/6559f5cfe251d64e7a7b49efb429ae5258eff95976e5f12312\n",
            "Successfully built tensorrt tensorrt-cu12\n",
            "Installing collected packages: tensorrt-cu12, tensorrt\n",
            "Successfully installed tensorrt-10.1.0 tensorrt-cu12-10.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "\n",
        "# Load ONNX model\n",
        "onnx_file_path = 'branched_cnn.onnx'\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "builder = trt.Builder(TRT_LOGGER)\n",
        "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "#network = builder.create_network(trt.NetworkDefinitionCreationFlags.EXPLICIT_BATCH)\n",
        "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "\n",
        "with open(onnx_file_path, 'rb') as model:\n",
        "    if not parser.parse(model.read()):\n",
        "        print('Failed to load ONNX file!')\n",
        "        for error in range(parser.num_errors):\n",
        "            print(parser.get_error(error))\n",
        "\n",
        "# Set precision for specific layers\n",
        "#builder.fp16_mode = True\n",
        "config = builder.create_builder_config()\n",
        "config.set_flag(trt.BuilderFlag.INT8)\n",
        "config.set_flag(trt.BuilderFlag.FP16)\n",
        "max_workspace_size = 1 << 30  # 1 GiB\n",
        "#config.max_workspace_size = max_workspace_size\n",
        "\n",
        "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)\n",
        "\n",
        "for layer_idx in range(network.num_layers):\n",
        "    layer = network.get_layer(layer_idx)\n",
        "    if isinstance(layer, trt.IConvolutionLayer):\n",
        "        # Check if the layer is part of the regular conv branch or the MobileNet branch\n",
        "        if \"mobilenet\" in layer.name:\n",
        "            layer.precision = trt.DataType.FLOAT  # FP32 for MobileNetV2 block\n",
        "            layer.set_output_type(0, trt.DataType.FLOAT)\n",
        "        else:\n",
        "            layer.precision = trt.DataType.INT8  # INT8 for regular conv block\n",
        "            layer.set_output_type(0, trt.DataType.INT8)\n",
        "\n",
        "# # Build the engine\n",
        "# engine = builder.build_engine(network, config)\n",
        "# #engine = builder.build_serialized_network(network, config)\n",
        "\n",
        "# # Save the engine to disk\n",
        "# with open('branched_cnn.trt', 'wb') as f:\n",
        "#     f.write(engine.serialize())\n",
        "\n",
        "# Build and serialize the engine (for TensorRT versions < 8.0)\n",
        "serialized_engine = builder.build_serialized_network(network, config)\n",
        "\n",
        "# Save the engine to disk\n",
        "with open('branched_cnn.trt', 'wb') as f:\n",
        "    f.write(serialized_engine)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "475MclHWamDT",
        "outputId": "60bf0aa1-23ea-4fa7-ae02-1293f0c470a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "a bytes-like object is required, not 'NoneType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3b9503ac6dfd>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Save the engine to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'branched_cnn.trt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1Cp6TKrd4C6",
        "outputId": "3534df2b-cbf4-4ad0-b7ea-01948966d8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.5-py2.py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs>=1.4.0 (from pycuda)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661206 sha256=fa56c065614c3ff0e24194daac718d822f85f6c56f36503c0af429a75b69f5f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n",
            "Successfully built pycuda\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.3.5 pycuda-2024.1 pytools-2024.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Load ONNX model\n",
        "onnx_file_path = 'branched_cnn.onnx'\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "builder = trt.Builder(TRT_LOGGER)\n",
        "network = builder.create_network(trt.NetworkDefinitionCreationFlags.EXPLICIT_BATCH)\n",
        "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "\n",
        "with open(onnx_file_path, 'rb') as model:\n",
        "    if not parser.parse(model.read()):\n",
        "        print('Failed to load ONNX file!')\n",
        "        for error in range(parser.num_errors):\n",
        "            print(parser.get_error(error))\n",
        "\n",
        "# Set precision for specific layers\n",
        "config = builder.create_builder_config()\n",
        "config.set_flag(trt.BuilderFlag.INT8)\n",
        "config.set_flag(trt.BuilderFlag.FP16)\n",
        "config.max_workspace_size = 1 << 30  # 1 GiB\n",
        "\n",
        "# Create a calibration cache for INT8 layers (needed for INT8 precision)\n",
        "def build_int8_calibrator():\n",
        "    # This is a placeholder function for building an INT8 calibrator.\n",
        "    # Implement this function based on your dataset.\n",
        "    pass\n",
        "\n",
        "int8_calibrator = build_int8_calibrator()\n",
        "if int8_calibrator:\n",
        "    config.int8_calibrator = int8_calibrator\n",
        "\n",
        "for layer_idx in range(network.num_layers):\n",
        "    layer = network.get_layer(layer_idx)\n",
        "    if isinstance(layer, trt.IConvolutionLayer):\n",
        "        # Check if the layer is part of the regular conv branch or the MobileNet branch\n",
        "        if \"mobilenet\" in layer.name:\n",
        "            layer.precision = trt.DataType.FLOAT  # FP32 for MobileNetV2 block\n",
        "            layer.set_output_type(0, trt.DataType.FLOAT)\n",
        "        else:\n",
        "            layer.precision = trt.DataType.INT8  # INT8 for regular conv block\n",
        "            layer.set_output_type(0, trt.DataType.INT8)\n",
        "\n",
        "# Build the serialized network\n",
        "serialized_engine = builder.build_serialized_network(network, config)\n",
        "\n",
        "# Deserialize the engine\n",
        "runtime = trt.Runtime(TRT_LOGGER)\n",
        "engine = runtime.deserialize_cuda_engine(serialized_engine)\n",
        "\n",
        "# Save the engine to disk\n",
        "with open('branched_cnn.trt', 'wb') as f:\n",
        "    f.write(serialized_engine)\n",
        "\n",
        "# Measure inference time\n",
        "h_input = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=np.float32)\n",
        "h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=np.float32)\n",
        "d_input = cuda.mem_alloc(h_input.nbytes)\n",
        "d_output = cuda.mem_alloc(h_output.nbytes)\n",
        "stream = cuda.Stream()\n",
        "\n",
        "context = engine.create_execution_context()\n",
        "\n",
        "np.copyto(h_input, np.random.rand(1, 3, 32, 32).ravel())\n",
        "\n",
        "# Measure inference time\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    cuda.memcpy_htod_async(d_input, h_input, stream)\n",
        "    context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)\n",
        "    cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
        "    stream.synchronize()\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Quantized model average inference time: {(end_time - start_time) / 100:.6f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "PezCcgOfdWs6",
        "outputId": "20f72dd7-20eb-463b-d6f9-4b6f08ce8c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorrt' has no attribute 'NetworkDefinitionCreationFlags'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-911dd83bf000>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mTRT_LOGGER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRT_LOGGER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkDefinitionCreationFlags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPLICIT_BATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOnnxParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRT_LOGGER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorrt' has no attribute 'NetworkDefinitionCreationFlags'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "\n",
        "# Load ONNX model\n",
        "onnx_file_path = 'branched_cnn.onnx'\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "builder = trt.Builder(TRT_LOGGER)\n",
        "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "\n",
        "with open(onnx_file_path, 'rb') as model:\n",
        "    if not parser.parse(model.read()):\n",
        "        print('Failed to load ONNX file!')\n",
        "        for error in range(parser.num_errors):\n",
        "            print(parser.get_error(error))\n",
        "\n",
        "# Set precision for specific layers\n",
        "config = builder.create_builder_config()\n",
        "config.set_flag(trt.BuilderFlag.INT8)\n",
        "config.set_flag(trt.BuilderFlag.FP16)\n",
        "max_workspace_size = 1 << 30  # 1 GiB\n",
        "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)\n",
        "\n",
        "for layer_idx in range(network.num_layers):\n",
        "    layer = network.get_layer(layer_idx)\n",
        "    if isinstance(layer, trt.IConvolutionLayer):\n",
        "        # Check if the layer is part of the regular conv branch or the MobileNet branch\n",
        "        if \"mobilenet\" in layer.name:\n",
        "            layer.precision = trt.DataType.FLOAT  # FP32 for MobileNetV2 block\n",
        "            layer.set_output_type(0, trt.DataType.FLOAT)\n",
        "        else:\n",
        "            layer.precision = trt.DataType.INT8  # INT8 for regular conv block\n",
        "            layer.set_output_type(0, trt.DataType.INT8)\n",
        "\n",
        "# Check for any unsupported layers and print warnings\n",
        "for layer_idx in range(network.num_layers):\n",
        "    layer = network.get_layer(layer_idx)\n",
        "    # Use is_int8 on the data type of the output tensor\n",
        "    if layer.get_output(0).dtype == trt.int8:\n",
        "        print(f\"Layer {layer.name} is using INT8 precision.\")\n",
        "    elif layer.get_output(0).dtype == trt.float16:\n",
        "        print(f\"Layer {layer.name} is using FP16 precision.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Layer {layer.name} is using FP32 precision. Consider using lower precision for better performance.\")\n",
        "\n",
        "# Build and serialize the engine (for TensorRT versions < 8.0)\n",
        "serialized_engine = builder.build_serialized_network(network, config)\n",
        "\n",
        "if serialized_engine is None:\n",
        "    print(\"ERROR: Failed to build TensorRT engine. Check for errors in the ONNX model or TensorRT configuration.\")\n",
        "else:\n",
        "    # Save the engine to disk\n",
        "    with open('branched_cnn.trt', 'wb') as f:\n",
        "        f.write(serialized_engine)\n",
        "    print(\"TensorRT engine built and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D-8BkvheXel",
        "outputId": "9cc3a370-c725-4e74-c947-9ff7716d771b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Layer /branch1/mobilenet_block/conv/conv.0/conv.0.0/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch1/mobilenet_block/conv/conv.0/conv.0.2/Clip is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch1/mobilenet_block/conv/conv.1/conv.1.0/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch1/mobilenet_block/conv/conv.1/conv.1.2/Clip is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch1/mobilenet_block/conv/conv.2/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch1/conv_block/conv_block.0/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch1/conv_block/conv_block.2/Relu is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch1/Add is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /pool/MaxPool is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/mobilenet_block/conv/conv.0/conv.0.0/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/mobilenet_block/conv/conv.0/conv.0.2/Clip is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/mobilenet_block/conv/conv.1/conv.1.0/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/mobilenet_block/conv/conv.1/conv.1.2/Clip is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/mobilenet_block/conv/conv.2/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/conv_block/conv_block.0/Conv is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/conv_block/conv_block.2/Relu is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /branch2/Add is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /pool_1/MaxPool is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /Reshape is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer fc1.weight is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /fc1/Gemm is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer fc1.bias is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer ONNXTRT_Broadcast is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer (Unnamed Layer* 23) [ElementWise] is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /Relu is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer fc2.weight is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /fc2/Gemm is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer fc2.bias is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer ONNXTRT_Broadcast_2 is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer (Unnamed Layer* 29) [ElementWise] is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /Relu_1 is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer fc3.weight is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer /fc3/Gemm is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer fc3.bias is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer ONNXTRT_Broadcast_4 is using FP32 precision. Consider using lower precision for better performance.\n",
            "WARNING: Layer (Unnamed Layer* 35) [ElementWise] is using FP32 precision. Consider using lower precision for better performance.\n",
            "ERROR: Failed to build TensorRT engine. Check for errors in the ONNX model or TensorRT configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZD_jnJifes0h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}