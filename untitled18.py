# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OJKizFBZcayYip07bKhQccCeBSmZRVLD
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Generate training data
x_train = np.linspace(-2, 2, 500)
y_train = np.exp(x_train)

# Create the MLP model
model = Sequential()
model.add(Dense(32, input_dim=1, activation='relu'))
#model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
history = model.fit(x_train, y_train, epochs=100, batch_size=10, verbose=0)

# Generate test data
x_test = np.linspace(-2, 2, 100)
y_test = np.exp(x_test)

# Predict using the model
y_pred = model.predict(x_test)

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(x_test, y_test, label='True e^x')
plt.plot(x_test, y_pred, label='MLP Prediction')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('MLP Approximation of e^x')
plt.show()

# prompt: write code to find out if the model prediction is faster than np.exp

import numpy as np
import time

# Calculate the time taken by the model prediction
start_time_model = time.time()
y_pred = model.predict(x_test)
end_time_model = time.time()
model_time = (end_time_model - start_time_model) * 1000

# Calculate the time taken by np.exp
start_time_np = time.time()
y_np = np.exp(x_test)
end_time_np = time.time()
np_time = (end_time_np - start_time_np) * 1000

# Compare the times
if model_time < np_time:
  print("Model prediction is faster than np.exp")
else:
  print("Model prediction is slower than np.exp")

# Print the times
print("Model prediction time:", model_time, "milliseconds")
print("np.exp time:", np_time, "milliseconds")

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import time

# Generate training data
x_train = np.linspace(-2, 2, 500).astype(np.float32).reshape(-1, 1)
y_train = np.exp(x_train).astype(np.float32).reshape(-1, 1)

# Convert to PyTorch tensors
x_train_tensor = torch.tensor(x_train)
y_train_tensor = torch.tensor(y_train)

# Define the MLP model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.hidden1 = nn.Linear(1, 64)
        self.hidden2 = nn.Linear(64, 64)
        self.output = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = self.output(x)
        return x

model = MLP()

# Define the loss function and the optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Train the model
epochs = 100
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(x_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Generate test data
x_test = np.linspace(-2, 2, 100).astype(np.float32).reshape(-1, 1)
x_test_tensor = torch.tensor(x_test)

# Measure prediction time for np.exp
start_time = time.time()
y_test = np.exp(x_test)
exp_time = time.time() - start_time

# Measure prediction time for the MLP model
model.eval()  # Set the model to evaluation mode
start_time = time.time()
with torch.no_grad():  # No need to compute gradients during inference
    y_pred_tensor = model(x_test_tensor)
    y_pred = y_pred_tensor.numpy()
mlp_time = time.time() - start_time

# Print the timing results
print(f"Time taken by np.exp: {exp_time:.6f} seconds")
print(f"Time taken by MLP model: {mlp_time:.6f} seconds")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(x_test, y_test, label='True e^x')
plt.plot(x_test, y_pred, label='MLP Prediction')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('MLP Approximation of e^x')
plt.show()

import numpy as np
import struct
import time

def exp(val):
    tmp = int(1512775 * val) + (1072693248 - 60801)
    return struct.unpack('d', struct.pack('Q', tmp << 32))[0]

# Generate test data
x_test = np.linspace(-2, 2, 100).astype(np.float32)

# Measure computation time for custom exp function
start_time = time.time()
y_custom_exp = np.array([exp(x) for x in x_test])
custom_exp_time = time.time() - start_time

# Measure computation time for np.exp function
start_time = time.time()
y_np_exp = np.exp(x_test)
np_exp_time = time.time() - start_time

# Compute the differences
differences = y_custom_exp - y_np_exp
max_difference = np.max(np.abs(differences))

# Print the timing and accuracy results
print(f"Time taken by custom exp function: {custom_exp_time:.6f} seconds")
print(f"Time taken by np.exp: {np_exp_time:.6f} seconds")
print(f"Maximum difference between custom exp and np.exp: {max_difference:.6f}")

# Plot the results
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(x_test, y_np_exp, label='np.exp')
plt.plot(x_test, y_custom_exp, label='Custom exp', linestyle='dashed')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('Comparison of np.exp and Custom exp Function')
plt.show()

import numpy as np
import struct
import time
import matplotlib.pyplot as plt

# Custom exp function
def exp(val):
    tmp = int(1512775 * val) + (1072693248 - 60801)
    return struct.unpack('d', struct.pack('Q', tmp << 32))[0]

# Smooth maximum function using the custom exp function
def smooth_max(x, beta=1.0):
    x = np.array(x, dtype=np.float32)
    exp_x = np.array([exp(beta * xi) for xi in x])
    return np.sum(x * exp_x) / np.sum(exp_x)

# Generate test data
x_test = np.random.uniform(-2, 2, 100)

# Measure computation time for custom smooth_max function
start_time = time.time()
smooth_max_value = smooth_max(x_test, beta=1.0)
custom_smooth_max_time = time.time() - start_time

# Measure computation time for np.max function
start_time = time.time()
np_max_value = np.max(x_test)
np_max_time = time.time() - start_time

# Compute the differences
difference = np.abs(smooth_max_value - np_max_value)

# Print the timing and accuracy results
print(f"Time taken by custom smooth_max function: {custom_smooth_max_time:.6f} seconds")
print(f"Time taken by np.max: {np_max_time:.6f} seconds")
print(f"Difference between custom smooth_max and np.max: {difference:.6f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(x_test, label='x values')
plt.axhline(y=smooth_max_value, color='r', linestyle='--', label='Smooth Max (Custom exp)')
plt.axhline(y=np_max_value, color='g', linestyle='--', label='Max (np.max)')
plt.legend()
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Comparison of Smooth Maximum and np.max')
plt.show()

import numpy as np
import struct
import time
import matplotlib.pyplot as plt
from concurrent.futures import ThreadPoolExecutor

# Custom exp function
def exp(val):
    tmp = int(1512775 * val) + (1072693248 - 60801)
    return struct.unpack('d', struct.pack('Q', tmp << 32))[0]

# Smooth maximum function using the custom exp function with parallel computation
def smooth_max(x, beta=1.0):
    x = np.array(x, dtype=np.float32)
    with ThreadPoolExecutor() as executor:
        exp_x = np.array(list(executor.map(lambda xi: exp(beta * xi), x)))
    return np.sum(x * exp_x) / np.sum(exp_x)

# Generate test data
x_test = np.random.uniform(-2, 2, 100)

# Measure computation time for custom smooth_max function
start_time = time.time()
smooth_max_value = smooth_max(x_test, beta=5.0)
custom_smooth_max_time = time.time() - start_time

# Measure computation time for np.max function
start_time = time.time()
np_max_value = np.max(x_test)
np_max_time = time.time() - start_time

# Compute the differences
difference = np.abs(smooth_max_value - np_max_value)

# Print the timing and accuracy results
print(f"Time taken by custom smooth_max function: {custom_smooth_max_time:.6f} seconds")
print(f"Time taken by np.max: {np_max_time:.6f} seconds")
print(f"Difference between custom smooth_max and np.max: {difference:.6f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(x_test, label='x values')
plt.axhline(y=smooth_max_value, color='r', linestyle='--', label='Smooth Max (Custom exp)')
plt.axhline(y=np_max_value, color='g', linestyle='--', label='Max (np.max)')
plt.legend()
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Comparison of Smooth Maximum and np.max')
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# Define the MLP to approximate the exponential function for a tensor of size 10
class ExpMLP(nn.Module):
    def __init__(self):
        super(ExpMLP, self).__init__()
        self.hidden1 = nn.Linear(10, 64)
        self.hidden2 = nn.Linear(64, 64)
        self.output = nn.Linear(64, 10)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = self.output(x)
        return x

# Generate training data for the ExpMLP
def generate_data(size=1000):
    x_train = np.random.uniform(-2, 2, (size, 10)).astype(np.float32)
    y_train = np.exp(x_train).astype(np.float32)
    return torch.tensor(x_train), torch.tensor(y_train)

# Create the ExpMLP model
exp_mlp = ExpMLP()

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(exp_mlp.parameters(), lr=0.01)

# Training the ExpMLP
def train_exp_mlp(model, criterion, optimizer, epochs=1000):
    model.train()
    x_train, y_train = generate_data()
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(x_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 10000 == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')

# Train the ExpMLP model
train_exp_mlp(exp_mlp, criterion, optimizer, epochs=100000)

# Example 1D tensor of 10 numbers
input_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=torch.float32)

# Forward pass through the trained MLP to calculate the exponential
exp_mlp.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient computation
    exp_mlp_output = exp_mlp(input_tensor.unsqueeze(0)).squeeze(0)

# Calculate actual exponential values
actual_exp_values = torch.exp(input_tensor)

# Calculate the difference
difference = exp_mlp_output - actual_exp_values

# Print Results
print("Input Tensor:")
print(input_tensor)
print("\nExponential Approximations (using MLP):")
print(exp_mlp_output)
print("\nActual Exponential Values:")
print(actual_exp_values)
print("\nDifference:")
print(difference)

# Plot the difference
plt.figure(figsize=(10, 6))
plt.plot(input_tensor.numpy(), difference.numpy(), marker='o')
plt.title('Difference between MLP Approximations and Actual Exponential Values')
plt.xlabel('Input Value')
plt.ylabel('Difference')
plt.grid(True)
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# Define the MLP to approximate the exponential function for a tensor of size 10
class ExpMLP(nn.Module):
    def __init__(self):
        super(ExpMLP, self).__init__()
        self.hidden1 = nn.Linear(10, 128)
        self.hidden2 = nn.Linear(128, 128)
        self.output = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = self.output(x)
        return x

# Generate training data for the ExpMLP
def generate_data(size=1000):
    x_train = np.random.uniform(-2, 2, (size, 10)).astype(np.float32)
    y_train = np.exp(x_train).astype(np.float32)
    return torch.tensor(x_train), torch.tensor(y_train)

# Create the ExpMLP model
exp_mlp = ExpMLP()

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(exp_mlp.parameters(), lr=0.01)

# Training the ExpMLP
def train_exp_mlp(model, criterion, optimizer, epochs=100):
    model.train()
    x_train, y_train = generate_data()
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(x_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 100 == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')

# Train the ExpMLP model
train_exp_mlp(exp_mlp, criterion, optimizer, epochs=1000)

# Example 1D tensor of 10 numbers within the range -2 to +2
input_tensor = torch.tensor([-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.0], dtype=torch.float32)

# Forward pass through the trained MLP to calculate the exponential
exp_mlp.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient computation
    exp_mlp_output = exp_mlp(input_tensor.unsqueeze(0)).squeeze(0)

# Calculate actual exponential values
actual_exp_values = torch.exp(input_tensor)

# Calculate the difference
difference = exp_mlp_output - actual_exp_values

# Print Results
print("Input Tensor:")
print(input_tensor)
print("\nExponential Approximations (using MLP):")
print(exp_mlp_output)
print("\nActual Exponential Values:")
print(actual_exp_values)
print("\nDifference:")
print(difference)

# Plot the difference
plt.figure(figsize=(10, 6))
plt.plot(input_tensor.numpy(), difference.numpy(), marker='o')
plt.title('Difference between MLP Approximations and Actual Exponential Values')
plt.xlabel('Input Value')
plt.ylabel('Difference')
plt.grid(True)
plt.show()

import torch
import torch.nn.functional as F

# Create a 3D tensor with dimensions (channels, width, height)
channels, width, height = 8, 4096, 50
input_tensor = torch.randn(channels, width, height)

# Calculate the softmax along the height (dimension 2)
softmax_output = F.softmax(input_tensor, dim=2)

print("Input Tensor Shape:", input_tensor.shape)
print("Softmax Output Tensor Shape:", softmax_output.shape)

import torch
import torch.nn.functional as F
import time

# Create a 3D tensor with dimensions (channels, width, height)
channels, width, height = 8, 4096, 50
input_tensor = torch.randn(channels, width, height)

# Choose a random value for a
a = torch.rand(1).item()

# Measure the time taken for a * e^x
start_time = time.time()
exp_output = torch.exp(input_tensor)
exp_time = time.time() - start_time

# Measure the time taken for softmax
start_time = time.time()
softmax_output = F.softmax(input_tensor, dim=2)
softmax_time = time.time() - start_time



print("Random value of a:", a)
print("Input Tensor Shape:", input_tensor.shape)
print("Softmax Output Tensor Shape:", softmax_output.shape)
print("Exponential Output Tensor Shape:", exp_output.shape)
print(f"Time taken by softmax: {softmax_time:.6f} seconds")
print(f"Time taken by a * e^x: {exp_time:.6f} seconds")

