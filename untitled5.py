# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10HZXUccr9di8GcyWwBpMxo36BqxZrZmO
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.utils.tensorboard import SummaryWriter
from torch.autograd import Variable
import numpy as np
import random

# Define the search space for the operations
ops = {
    'conv_3x3': lambda C_in, C_out, stride: nn.Conv2d(C_in, C_out, kernel_size=3, stride=stride, padding=1),
    'conv_5x5': lambda C_in, C_out, stride: nn.Conv2d(C_in, C_out, kernel_size=5, stride=stride, padding=2),
    'pool_3x3': lambda C_in, C_out, stride: nn.MaxPool2d(kernel_size=3, stride=stride, padding=1)
}

class Cell(nn.Module):
    def __init__(self, ops):
        super(Cell, self).__init__()
        self.ops = nn.ModuleList()
        for key in ops:
            self.ops.append(ops[key](16, 16, 1))
        self.weights = nn.Parameter(torch.Tensor(len(ops)))
        self.weights.data.fill_(1/len(ops))

    def forward(self, x):
        res = []
        for i in range(len(self.ops)):
            res.append(self.ops[i](x))
        return sum(w * op_res for w, op_res in zip(self.weights, res))

class CNN(nn.Module):
    def __init__(self, ops):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.cells = nn.ModuleList()
        for i in range(2):
            cell = Cell(ops)
            self.cells.append(cell)
        self.conv2 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        x = self.conv1(x)
        for cell in self.cells:
            x = cell(x)
        x = self.conv2(x)
        x = nn.functional.avg_pool2d(x, 28)
        x = x.view(x.size(0), -1)
        return nn.functional.log_softmax(x, dim=1)

# Define the training loop
def train(model, device, train_loader, optimizer, criterion, epoch, log_interval):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))

# Define the test loop
def test(model, device, test_loader, criterion):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-pro
            correct += pred.eq(target.view_as(pred)).sum().item()
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
    test_loss, correct, len(test_loader.dataset),
    100. * correct / len(test_loader.dataset)))
    return test_loss,correct

# Define the dummy dataset
class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, num_samples=1000, img_size=28, num_classes=10):
        self.num_samples = num_samples
        self.img_size = img_size
        self.num_classes = num_classes
        self.samples = []
        for i in range(num_samples):
            img = np.zeros((img_size, img_size))
            label = random.randint(0, num_classes-1)
            x1 = random.randint(0, img_size//2)
            y1 = random.randint(0, img_size//2)
            x2 = random.randint(img_size//2, img_size-1)
            y2 = random.randint(img_size//2, img_size-1)
            img[x1:x2, y1:y2] = label + 1
            self.samples.append((torch.Tensor(img).unsqueeze(0), label))

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return self.samples[idx]

# Define the data loaders
train_dataset = DummyDataset()
test_dataset = DummyDataset(num_samples=200, img_size=28, num_classes=10)

batch_size = 32
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Set up the device and model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNN(ops).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# Set up the logger
writer = SummaryWriter()

# Train the model
epochs = 5
log_interval = 10
for epoch in range(1, epochs + 1):
    train(model, device, train_loader, optimizer, criterion, epoch, log_interval)
    test_loss, test_acc = test(model, device, test_loader, criterion)
    scheduler.step()
    writer.add_scalar('Test Loss', test_loss, epoch)
    writer.add_scalar('Test Accuracy', test_acc, epoch)

# Close the logger
writer.close()

epochs = 50
log_interval = 10
for epoch in range(1, epochs + 1):
    train(model, device, train_loader, optimizer, criterion, epoch, log_interval)
    test_loss, test_acc = test(model, device, test_loader, criterion)
    scheduler.step()
    writer.add_scalar('Test Loss', test_loss, epoch)
    writer.add_scalar('Test Accuracy', test_acc, epoch)

# Close the logger
writer.close()

print(model)

for name, param in model.named_parameters():
    if 'weights' in name:
      print(param)

