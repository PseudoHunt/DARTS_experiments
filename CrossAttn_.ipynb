{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, key_value_dim, embed_size, heads):\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.queries = nn.Linear(query_dim, embed_size, bias=False)\n",
        "        self.keys = nn.Linear(key_value_dim, embed_size, bias=False)\n",
        "        self.values = nn.Linear(key_value_dim, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, queries, keys, values, mask):\n",
        "        N = queries.shape[0]\n",
        "        query_len = queries.shape[1]\n",
        "        key_len = keys.shape[1]\n",
        "\n",
        "        # Linear projections\n",
        "        queries_proj = self.queries(queries)\n",
        "        keys_proj = self.keys(keys)\n",
        "        values_proj = self.values(values)\n",
        "\n",
        "        # Reshape into multiple heads\n",
        "        queries_proj = queries_proj.reshape(N, query_len, self.heads, self.head_dim)\n",
        "        keys_proj = keys_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(N, 1, 1, 1)\n",
        "        values_proj = values_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(N, 1, 1, 1)\n",
        "\n",
        "        # Permute to bring heads dimension in front\n",
        "        queries_proj = queries_proj.permute(0, 2, 1, 3)  # Shape: [N, heads, query_len, head_dim]\n",
        "        keys_proj = keys_proj.permute(0, 2, 1, 3)        # Shape: [N, heads, key_len, head_dim]\n",
        "        values_proj = values_proj.permute(0, 2, 1, 3)    # Shape: [N, heads, key_len, head_dim]\n",
        "\n",
        "        # Step 1: Reshape queries and keys for batched matrix multiplication\n",
        "        queries_proj = queries_proj.reshape(N * self.heads, query_len, self.head_dim)\n",
        "        keys_proj = keys_proj.reshape(N * self.heads, key_len, self.head_dim)\n",
        "\n",
        "        # Step 2: Matrix multiplication (batch matmul)\n",
        "        energy = torch.matmul(queries_proj, keys_proj.transpose(-1, -2))  # Shape: [N * self.heads, query_len, key_len]\n",
        "\n",
        "        # Step 3: Reshape back to original shape\n",
        "        energy = energy.reshape(N, self.heads, query_len, key_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        values_proj = values_proj.reshape(N * self.heads, key_len, self.head_dim)\n",
        "        out = torch.matmul(attention.reshape(N * self.heads, query_len, key_len), values_proj)\n",
        "\n",
        "        # Reshape to (N, query_len, heads, head_dim) and combine heads\n",
        "        out = out.reshape(N, self.heads, query_len, self.head_dim).permute(0, 2, 1, 3).reshape(N, query_len, self.embed_size)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "query_dim = 320\n",
        "key_value_dim = 768\n",
        "embed_size = 320\n",
        "heads = 8\n",
        "queries = torch.rand((64, 64, query_dim))\n",
        "keys = torch.rand((1, 50, key_value_dim))\n",
        "values = torch.rand((1, 50, key_value_dim))\n",
        "mask = None\n",
        "\n",
        "cross_attention_layer = MultiHeadCrossAttention(query_dim, key_value_dim, embed_size, heads)\n",
        "out = cross_attention_layer(queries, keys, values, mask)\n",
        "print(out.shape)  # Should print torch.Size([64, 64, 320])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjlSXGJgqoJj",
        "outputId": "62ba7f33-589f-4a5e-b5d7-3173147f2185"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 64, 320])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, key_value_dim, embed_size, heads):\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.queries = nn.Linear(query_dim, embed_size, bias=False)\n",
        "        self.keys = nn.Linear(key_value_dim, embed_size, bias=False)\n",
        "        self.values = nn.Linear(key_value_dim, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def preprocess(self,keys,queries,values):\n",
        "        #iNIT\n",
        "        N = queries.shape[0]\n",
        "        query_len = queries.shape[1]\n",
        "        print(self.keys.weight.shape)\n",
        "\n",
        "        key_len = keys.shape[1]\n",
        "        keys_proj = self.keys(keys)\n",
        "        print(keys_proj.shape) # 1,50,320\n",
        "        keys_proj = keys_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(1, 1, 1, 1)\n",
        "        keys_proj = keys_proj.permute(0, 2, 3, 1)  # Shape: [N, heads, head_dim,key_len]\n",
        "        print(keys_proj.shape) # 1,8,40,50\n",
        "        Wq = self.queries.weight\n",
        "        #Wq = Wq.reshape(1, self.embed_size, self.heads, self.head_dim)\n",
        "        Wq = Wq.reshape(1, self.heads, self.head_dim,self.embed_size)\n",
        "        print(Wq.shape) # 320,8,40\n",
        "        Wq = Wq.permute(0, 1, 3, 2)  # Shape: [N, heads, key_len, head_dim]\n",
        "        print(Wq.shape) # 1,8,320,40\n",
        "        self.qk = torch.matmul(Wq, keys_proj)\n",
        "        print(self.qk.shape)\n",
        "        #self.qk = self.queries(keys_proj)\n",
        "        #self.qk = self.qk.reshape(1, key_len, self.heads, self.head_dim).repeat(1, 1, 1, 1)\n",
        "        #print(self.qk.shape)\n",
        "\n",
        "        #computation\n",
        "        energy = torch.matmul(queries,self.qk)  # Shape: [N * self.heads, query_len, key_len]\n",
        "        print(energy.shape)\n",
        "\n",
        "        # Step 3: Reshape back to original shape\n",
        "        energy = energy.reshape(N, self.heads, query_len, key_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
        "        #until here it works\n",
        "\n",
        "        #Fuse V and out\n",
        "        values_proj = self.values(values)\n",
        "        print(values_proj.shape) # 1,50,320\n",
        "        values_proj = values_proj.reshape(1, key_len, self.heads, self.head_dim)\n",
        "        print(values_proj.shape) # 1,50,8,40\n",
        "        values_proj = values_proj.permute(0, 2, 1, 3)  # Shape: [N, heads, key_len, head_dim]\n",
        "        print(values_proj.shape) # 1,8,50,40\n",
        "        W_out = self.fc_out.weight.T\n",
        "        #method1\n",
        "        W_out = W_out.reshape(1, self.heads, self.head_dim,self.embed_size)\n",
        "        print(W_out.shape) # 1,8,40,320\n",
        "        #method2\n",
        "        #W_out = W_out.reshape(1, self.embed_size, self.heads, self.head_dim)\n",
        "        #print(W_out.shape) # 1,320,8,40\n",
        "        #W_out = W_out.permute(0, 2, 3, 1)  # Shape: [N, heads, head_dim,key_len]\n",
        "        #print(W_out.shape) # 1,8,40,320\n",
        "        #multiply values_proj and W_out\n",
        "        out = torch.matmul(values_proj,W_out)\n",
        "        print(out.shape)\n",
        "        print('check above')\n",
        "\n",
        "        #compute\n",
        "        out = torch.matmul(attention,out)\n",
        "        print(out.shape)\n",
        "        out = torch.sum(out, dim=1)\n",
        "        out += self.fc_out.bias\n",
        "        print(out.shape)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def precompute(self):\n",
        "        #iNIT\n",
        "        N = queries.shape[0]\n",
        "        query_len = queries.shape[1]\n",
        "        print(self.keys.weight.shape)\n",
        "\n",
        "        #Fuse Q and K\n",
        "        key_len = keys.shape[1]\n",
        "        keys_proj = self.keys(keys)\n",
        "        self.keys_proj = keys_proj\n",
        "        keys_proj = keys_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(1, 1, 1, 1)\n",
        "        keys_proj = keys_proj.permute(0, 2, 3, 1)  # Shape: [N, heads, head_dim,key_len]\n",
        "        Wq = self.queries.weight\n",
        "        Wq = Wq.reshape(1, self.heads, self.head_dim,self.embed_size)\n",
        "        Wq = Wq.permute(0, 1, 3, 2)  # Shape: [N, heads, key_len, head_dim]\n",
        "        self.qk = torch.matmul(Wq, keys_proj)\n",
        "        #print(self.qk.shape)\n",
        "        #print('self.qk.shape')\n",
        "\n",
        "        #Fuse V and out\n",
        "        values_proj = self.values(values)\n",
        "        values_proj = values_proj.reshape(1, key_len, self.heads, self.head_dim)\n",
        "        values_proj = values_proj.permute(0, 2, 1, 3)  # Shape: [N, heads, key_len, head_dim]\n",
        "        W_out = self.fc_out.weight.T\n",
        "        W_out = W_out.reshape(1, self.heads, self.head_dim,self.embed_size)\n",
        "        self.Vout = torch.matmul(values_proj,W_out)\n",
        "\n",
        "    def fused_fwd(self,queries):\n",
        "        #init\n",
        "        N = queries.shape[0]\n",
        "        query_len = queries.shape[1]\n",
        "        key_len = keys.shape[1]\n",
        "        queries = queries.unsqueeze(1).repeat(1, 8, 1, 1)\n",
        "        #computation qk\n",
        "        #print('fused shape check')\n",
        "        #print(queries.shape)\n",
        "        #print(self.qk.shape)\n",
        "        #print('end')\n",
        "        energy = torch.matmul(queries,self.qk)  # Shape: [N * self.heads, query_len, key_len]\n",
        "        energy = energy.reshape(N, self.heads, query_len, key_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        #print(energy.shape)\n",
        "        #print('energy fused')\n",
        "\n",
        "        #compute attention mask\n",
        "        attention = torch.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
        "\n",
        "        #compute out\n",
        "        out = torch.matmul(attention,self.Vout)\n",
        "        out = torch.sum(out, dim=1)\n",
        "        out += self.fc_out.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "    def fused_sec_half(self, queries, keys, values, mask):\n",
        "        N = queries.shape[0]\n",
        "        query_len = queries.shape[1]\n",
        "        key_len = keys.shape[1]\n",
        "\n",
        "        # Linear projections\n",
        "        queries_proj = self.queries(queries)\n",
        "        #keys_proj = self.keys(keys)\n",
        "        keys_proj = self.keys_proj\n",
        "        values_proj = self.values(values)\n",
        "\n",
        "        # Reshape into multiple heads\n",
        "        queries_proj = queries_proj.reshape(N, query_len, self.heads, self.head_dim)\n",
        "        keys_proj = keys_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(N, 1, 1, 1)\n",
        "        values_proj = values_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(N, 1, 1, 1)\n",
        "\n",
        "        # Permute to bring heads dimension in front\n",
        "        queries_proj = queries_proj.permute(0, 2, 1, 3)  # Shape: [N, heads, query_len, head_dim]\n",
        "        keys_proj = keys_proj.permute(0, 2, 1, 3)        # Shape: [N, heads, key_len, head_dim]\n",
        "        values_proj = values_proj.permute(0, 2, 1, 3)    # Shape: [N, heads, key_len, head_dim]\n",
        "\n",
        "        # Step 1: Reshape queries and keys for batched matrix multiplication\n",
        "        queries_proj = queries_proj.reshape(N * self.heads, query_len, self.head_dim)\n",
        "        keys_proj = keys_proj.reshape(N * self.heads, key_len, self.head_dim)\n",
        "\n",
        "        # Step 2: Matrix multiplication (batch matmul)\n",
        "        #print('original check')\n",
        "        #print(queries_proj.shape)\n",
        "        #print(keys_proj.shape)\n",
        "        energy = torch.matmul(queries_proj, keys_proj.transpose(-1, -2))  # Shape: [N * self.heads, query_len, key_len]\n",
        "\n",
        "        # Step 3: Reshape back to original shape\n",
        "        energy = energy.reshape(N, self.heads, query_len, key_len)\n",
        "        #return energy\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        #print(energy.shape)\n",
        "        #print('energy')\n",
        "        attention = torch.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
        "        #return attention\n",
        "        # Apply attention weights to values\n",
        "        #compute out\n",
        "        #print('sec half mlu')\n",
        "        #print(attention.shape)\n",
        "        #print(self.Vout.shape)\n",
        "        out = torch.matmul(attention,self.Vout)\n",
        "        out = torch.sum(out, dim=1)\n",
        "        out += self.fc_out.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, queries, keys, values, mask):\n",
        "        N = queries.shape[0]\n",
        "        query_len = queries.shape[1]\n",
        "        key_len = keys.shape[1]\n",
        "\n",
        "        # Linear projections\n",
        "        queries_proj = self.queries(queries)\n",
        "        keys_proj = self.keys(keys)\n",
        "        values_proj = self.values(values)\n",
        "\n",
        "        # Reshape into multiple heads\n",
        "        queries_proj = queries_proj.reshape(N, query_len, self.heads, self.head_dim)\n",
        "        keys_proj = keys_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(N, 1, 1, 1)\n",
        "        values_proj = values_proj.reshape(1, key_len, self.heads, self.head_dim).repeat(N, 1, 1, 1)\n",
        "\n",
        "        # Permute to bring heads dimension in front\n",
        "        queries_proj = queries_proj.permute(0, 2, 1, 3)  # Shape: [N, heads, query_len, head_dim]\n",
        "        keys_proj = keys_proj.permute(0, 2, 1, 3)        # Shape: [N, heads, key_len, head_dim]\n",
        "        values_proj = values_proj.permute(0, 2, 1, 3)    # Shape: [N, heads, key_len, head_dim]\n",
        "\n",
        "        # Step 1: Reshape queries and keys for batched matrix multiplication\n",
        "        queries_proj = queries_proj.reshape(N * self.heads, query_len, self.head_dim)\n",
        "        keys_proj = keys_proj.reshape(N * self.heads, key_len, self.head_dim)\n",
        "\n",
        "        # Step 2: Matrix multiplication (batch matmul)\n",
        "        #print('original check')\n",
        "        #print(queries_proj.shape)\n",
        "        #print(keys_proj.shape)\n",
        "        energy = torch.matmul(queries_proj, keys_proj.transpose(-1, -2))  # Shape: [N * self.heads, query_len, key_len]\n",
        "\n",
        "        # Step 3: Reshape back to original shape\n",
        "        energy = energy.reshape(N, self.heads, query_len, key_len)\n",
        "        #return energy\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        #print(energy.shape)\n",
        "        #print('energy')\n",
        "        attention = torch.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
        "        #return attention\n",
        "        # Apply attention weights to values\n",
        "        values_proj = values_proj.reshape(N * self.heads, key_len, self.head_dim)\n",
        "        out = torch.matmul(attention.reshape(N * self.heads, query_len, key_len), values_proj)\n",
        "\n",
        "        # Reshape to (N, query_len, heads, head_dim) and combine heads\n",
        "        out = out.reshape(N, self.heads, query_len, self.head_dim).permute(0, 2, 1, 3).reshape(N, query_len, self.embed_size)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "query_dim = 320\n",
        "key_value_dim = 768\n",
        "embed_size = 320\n",
        "heads = 8\n",
        "queries = torch.rand((1, 4096, query_dim))\n",
        "keys = torch.rand((1, 50, key_value_dim))\n",
        "values = keys\n",
        "#values = torch.rand((1, 50, key_value_dim))\n",
        "mask = None\n",
        "\n",
        "\n",
        "cross_attention_layer = MultiHeadCrossAttention(query_dim, key_value_dim, embed_size, heads)\n",
        "#test = cross_attention_layer.preprocess(keys,queries,values)\n",
        "out1 = cross_attention_layer(queries, keys, values, mask)\n",
        "print(out1.shape)  # Should print torch.Size([64, 64, 320])\n",
        "cross_attention_layer.precompute()\n",
        "out2 = cross_attention_layer.fused_sec_half(queries, keys, values, mask)\n",
        "print(out2.shape)  # Should print torch.Size([64, 64, 320])\n",
        "\n",
        "\n",
        "out = cross_attention_layer.fused_fwd(queries)\n",
        "print(out.shape)  # Should print torch.Size([64, 64, 320])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osVW0IKOspNK",
        "outputId": "162dc00c-52d9-46cd-c8d0-02316f098c1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4096, 320])\n",
            "torch.Size([320, 768])\n",
            "torch.Size([1, 4096, 320])\n",
            "torch.Size([1, 4096, 320])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: compute time taken for fused_fwd and forward function on gpu\n",
        "\n",
        "import time\n",
        "\n",
        "# Move tensors to GPU\n",
        "queries = queries.cuda()\n",
        "keys = keys.cuda()\n",
        "values = values.cuda()\n",
        "#mask = mask.cuda()\n",
        "\n",
        "cross_attention_layer.cuda()\n",
        "cross_attention_layer.precompute()\n",
        "\n",
        "# Time forward\n",
        "start_time = time.time()\n",
        "out_forward = cross_attention_layer(queries, keys, values, mask)\n",
        "end_time = time.time()\n",
        "forward_time = end_time - start_time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Time fused_fwd\n",
        "start_time = time.time()\n",
        "out_fused = cross_attention_layer.fused_fwd(queries)\n",
        "end_time = time.time()\n",
        "fused_fwd_time = end_time - start_time\n",
        "\n",
        "\n",
        "# Time forward\n",
        "start_time = time.time()\n",
        "out_forward = cross_attention_layer.fused_sec_half(queries, keys, values, mask)\n",
        "end_time = time.time()\n",
        "sec_half_time = end_time - start_time\n",
        "\n",
        "\n",
        "print(\"Normal Forward time:\", forward_time)\n",
        "print(\"Fused_fwd time:\", fused_fwd_time)\n",
        "print(\"sec half fused time:\", sec_half_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U3ZTW7VV6mM",
        "outputId": "29ab85e9-a016-49ee-9d92-3f128f8a702d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([320, 768])\n",
            "Normal Forward time: 0.0011096000671386719\n",
            "Fused_fwd time: 0.0005753040313720703\n",
            "sec half fused time: 0.0008502006530761719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: compute time taken for fused_fwd and forward function and fused_sec_half for hundred runs on gpu in millisecond\n",
        "\n",
        "import time\n",
        "\n",
        "# Move tensors to GPU\n",
        "queries = queries.cuda()\n",
        "keys = keys.cuda()\n",
        "values = values.cuda()\n",
        "#mask = mask.cuda()\n",
        "\n",
        "cross_attention_layer.cuda()\n",
        "cross_attention_layer.precompute()\n",
        "\n",
        "# Time fused_fwd\n",
        "fused_fwd_times = []\n",
        "for i in range(100):\n",
        "  start_time = time.time()\n",
        "  out_fused = cross_attention_layer.fused_fwd(queries)\n",
        "  end_time = time.time()\n",
        "  fused_fwd_times.append((end_time - start_time) * 1000)\n",
        "\n",
        "# Time forward\n",
        "forward_times = []\n",
        "for i in range(100):\n",
        "  start_time = time.time()\n",
        "  out_forward = cross_attention_layer.forward(queries, keys, values, mask)\n",
        "  end_time = time.time()\n",
        "  forward_times.append((end_time - start_time) * 1000)\n",
        "\n",
        "# Time fused_sec_half\n",
        "sec_half_times = []\n",
        "for i in range(100):\n",
        "  start_time = time.time()\n",
        "  out_forward = cross_attention_layer.fused_sec_half(queries, keys, values, mask)\n",
        "  end_time = time.time()\n",
        "  sec_half_times.append((end_time - start_time) * 1000)\n",
        "\n",
        "print(\"Average Fused_fwd time:\", sum(fused_fwd_times) / len(fused_fwd_times))\n",
        "print(\"Average Forward time:\", sum(forward_times) / len(forward_times))\n",
        "print(\"Average sec half fused time:\", sum(sec_half_times) / len(sec_half_times))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcCqVKLkeu24",
        "outputId": "72c70001-31e6-471f-fefb-8c2b447ac7b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([320, 768])\n",
            "Average Fused_fwd time: 0.21575689315795898\n",
            "Average Forward time: 2.0983076095581055\n",
            "Average sec half fused time: 0.7781839370727539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDCH-JWQmm2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}