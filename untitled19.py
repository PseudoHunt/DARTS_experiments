# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q3WDiJCCtsd8NzFmq1b5IbnHjju38mVi
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18
import time

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define transforms for the training and testing sets
transform = transforms.Compose([
    transforms.Resize(224),  # Resize images to 224x224 as expected by ResNet
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

# Load the pre-trained ResNet-18 model
model = resnet18(pretrained=True)

# Modify the final layer to fit CIFAR-10 classes using an MLP
num_ftrs = model.fc.in_features

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model.fc = MLP(input_dim=num_ftrs, hidden_dim=512, output_dim=10)
model = model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)



# Testing the model and measuring the softmax computation time
def test(model, testloader, device):
    model.eval()
    softmax = nn.Softmax(dim=1)
    correct = 0
    total = 0
    total_softmax_time = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)

            # Get the output of the model
            outputs = model(images)

            # Measure softmax computation time
            start_time = time.time()
            softmax_outputs = softmax(outputs)
            softmax_time = time.time() - start_time
            total_softmax_time += softmax_time

            _, predicted = torch.max(softmax_outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            break

    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
    print(f'Total time taken by softmax computation: {total_softmax_time:.6f} seconds')


# Test the model and measure softmax computation time
test(model, testloader, device)

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18
import time
import numpy as np

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define transforms for the training and testing sets
transform = transforms.Compose([
    transforms.Resize(224),  # Resize images to 224x224 as expected by ResNet
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

# Load the pre-trained ResNet-18 model
model = resnet18(pretrained=True)

# Modify the final layer to fit CIFAR-10 classes using an MLP
num_ftrs = model.fc.in_features

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Define a small MLP to approximate the exponential function
class ExpMLP(nn.Module):
    def __init__(self):
        super(ExpMLP, self).__init__()
        self.hidden1 = nn.Linear(10, 64)
        #self.hidden2 = nn.Linear(128, 128)
        self.output = nn.Linear(64, 10)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        #x = torch.relu(self.hidden2(x))
        x = self.output(x)
        return x

# Generate training data for the ExpMLP
x_train = np.linspace(-2, 2, 500).astype(np.float32).reshape(-1, 1)
y_train = np.exp(x_train).astype(np.float32).reshape(-1, 1)

# Convert to PyTorch tensors
x_train_tensor = torch.tensor(x_train)
y_train_tensor = torch.tensor(y_train)

# Train the ExpMLP
exp_mlp = ExpMLP().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(exp_mlp.parameters(), lr=0.01)
epochs = 100
# exp_mlp.train()
# for epoch in range(epochs):
#     optimizer.zero_grad()
#     outputs = exp_mlp(x_train_tensor.to(device))
#     loss = criterion(outputs, y_train_tensor.to(device))
#     loss.backward()
#     optimizer.step()
#     if (epoch + 1) % 10 == 0:
#         print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')

# Use the trained ExpMLP in the ResNet model
model.fc = MLP(input_dim=num_ftrs, hidden_dim=512, output_dim=10)
model = model.to(device)



# Testing the ResNet model and measuring the time taken by the ExpMLP
def test(model, testloader, exp_mlp, device):
    model.eval()
    exp_mlp.eval()
    correct = 0
    total = 0
    total_exp_mlp_time = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)

            # Get the output of the ResNet model
            outputs = model(images)

            # Measure the time taken by the ExpMLP
            start_time = time.time()
            #exp_outputs = torch.exp(outputs)
            exp_mlp_outputs = exp_mlp(outputs)
            #exp_mlp_outputs = exp_mlp_outputs.view(outputs.shape)
            exp_mlp_time = time.time() - start_time
            total_exp_mlp_time += exp_mlp_time

            # Get the predicted class
            _, predicted = torch.max(exp_mlp_outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            break

    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
    print(f'Total time taken by ExpMLP computation: {total_exp_mlp_time:.6f} seconds')



# Test the ResNet model and measure the time taken by the ExpMLP
test(model, testloader, exp_mlp, device)

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18
import time
import numpy as np

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define transforms for the training and testing sets
transform = transforms.Compose([
    transforms.Resize(224),  # Resize images to 224x224 as expected by ResNet
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

# Load the pre-trained ResNet-18 model
model = resnet18(pretrained=True)

# Modify the final layer to fit CIFAR-10 classes using an MLP
num_ftrs = model.fc.in_features

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Define a small MLP to approximate the exponential function
class ExpMLP(nn.Module):
    def __init__(self):
        super(ExpMLP, self).__init__()
        self.hidden1 = nn.Linear(1, 64)
        self.hidden2 = nn.Linear(64, 64)
        self.output = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = self.output(x)
        return x

# Generate training data for the ExpMLP
x_train = np.linspace(-2, 2, 500).astype(np.float32).reshape(-1, 1)
y_train = np.exp(x_train).astype(np.float32).reshape(-1, 1)

# Convert to PyTorch tensors
x_train_tensor = torch.tensor(x_train)
y_train_tensor = torch.tensor(y_train)

# Train the ExpMLP
exp_mlp = ExpMLP().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(exp_mlp.parameters(), lr=0.01)
epochs = 100
exp_mlp.train()
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = exp_mlp(x_train_tensor.to(device))
    loss = criterion(outputs, y_train_tensor.to(device))
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')

# Use the trained ExpMLP in the ResNet model
model.fc = MLP(input_dim=num_ftrs, hidden_dim=512, output_dim=10)
model = model.to(device)



# Testing the ResNet model and measuring the time taken by the ExpMLP
def test(model, testloader, exp_mlp, device):
    model.eval()
    exp_mlp.eval()
    correct = 0
    total = 0
    total_exp_mlp_time = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)

            # Get the output of the ResNet model
            outputs = model(images)

            # Measure the time taken by the ExpMLP
            start_time = time.time()
            exp_outputs = torch.exp(outputs)
            #exp_mlp_outputs = exp_mlp(outputs.view(-1, 1))
            #exp_mlp_outputs = exp_mlp_outputs.view(outputs.shape)
            exp_mlp_time = time.time() - start_time
            total_exp_mlp_time += exp_mlp_time

            # Get the predicted class
            _, predicted = torch.max(exp_outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            break

    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
    print(f'Total time taken by ExpMLP computation: {total_exp_mlp_time:.6f} seconds')


# Test the ResNet model and measure the time taken by the ExpMLP
test(model, testloader, exp_mlp, device)

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18
import time
import numpy as np

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define transforms for the training and testing sets
transform = transforms.Compose([
    transforms.Resize(224),  # Resize images to 224x224 as expected by ResNet
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

# Load the pre-trained ResNet-18 model
model = resnet18(pretrained=True)

# Modify the final layer to fit CIFAR-10 classes using an MLP
num_ftrs = model.fc.in_features

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Define a small MLP to approximate the exponential function
class ExpMLP(nn.Module):
    def __init__(self):
        super(ExpMLP, self).__init__()
        self.hidden1 = nn.Linear(10, 1)
        #self.hidden2 = nn.Linear(64, 64)
        #self.output = nn.Linear(64, 1)

    def forward(self, x):
        x_sum = self.hidden1(x)
        x = x/x_sum
        #x = self.output(x)
        return x

# Generate training data for the ExpMLP
x_train = np.linspace(-2, 2, 500).astype(np.float32).reshape(-1, 1)
y_train = np.exp(x_train).astype(np.float32).reshape(-1, 1)

# Convert to PyTorch tensors
x_train_tensor = torch.tensor(x_train)
y_train_tensor = torch.tensor(y_train)



# Use the trained ExpMLP in the ResNet model
model.fc = MLP(input_dim=num_ftrs, hidden_dim=512, output_dim=10)
model = model.to(device)



# Testing the ResNet model and measuring the time taken by the ExpMLP
def test(model, testloader, exp_mlp, device):
    model.eval()
    exp_mlp.eval()
    correct = 0
    total = 0
    total_exp_mlp_time = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)

            # Get the output of the ResNet model
            outputs = model(images)

            # Measure the time taken by the ExpMLP
            start_time = time.time()
            exp_outputs = torch.exp(outputs)
            #exp_mlp_outputs = exp_mlp(outputs.view(-1, 1))
            #exp_mlp_outputs = exp_mlp_outputs.view(outputs.shape)
            exp_mlp_time = time.time() - start_time
            total_exp_mlp_time += exp_mlp_time

            # Get the predicted class
            _, predicted = torch.max(exp_outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            break

    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
    print(f'Total time taken by ExpMLP computation: {total_exp_mlp_time:.6f} seconds')


# Test the ResNet model and measure the time taken by the ExpMLP
test(model, testloader, exp_mlp, device)

import torch
import torch.nn.functional as F

def piecewise_linear_exp(x):
    """Approximate exp(x) using a piecewise linear function with ReLU and Leaky ReLU."""
    # Define piecewise linear functions using ReLU and Leaky ReLU
    exp_approx = torch.zeros_like(x)

    # Segment for x < -1
    exp_approx += F.leaky_relu(0.1 * x + 0.3679, negative_slope=0.1)  # Leaky ReLU with small slope

    # Segment for -1 <= x < 1
    exp_approx += F.relu(0.5 * x + 1) - F.relu(0.5 * x + 1 - 1.3679)  # ReLU adjusted for range

    # Segment for x >= 1
    exp_approx += F.relu(1.2 * x + 0.8 - 1.3679) + 1.3679  # ReLU shifted for range

    return exp_approx

def softmax_piecewise_linear(x):
    """Approximate softmax using piecewise linear approximation for exp(x)."""
    exp_approx = piecewise_linear_exp(x)
    return exp_approx / torch.sum(exp_approx, dim=-1, keepdim=True)

# Test the approximation
x = torch.randn(1, 5)  # Example input tensor
approx_softmax = softmax_piecewise_linear(x)
true_softmax = torch.nn.functional.softmax(x, dim=-1)

print("Input Tensor:", x)
print("Approximated Softmax:", approx_softmax)
print("True Softmax:", true_softmax)
print("Difference:", torch.abs(true_softmax - approx_softmax))

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Define the model
class LeakyReLUFit(nn.Module):
    def __init__(self):
        super(LeakyReLUFit, self).__init__()
        self.a = nn.Parameter(torch.randn(1))
        self.b = nn.Parameter(torch.randn(1))
        #self.b = 1.0
        self.negative_slope = nn.Parameter(torch.tensor(0.1))  # Initialize with a small positive value

    def forward(self, x):
        return self.a * (torch.relu(x) - self.negative_slope * torch.relu(-x)) + self.b

# Generate data
x = torch.linspace(-2, 2, 100).unsqueeze(1)  # 100 points from -2 to 2
y = torch.exp(x)

# Initialize the model, loss function, and optimizer
model = LeakyReLUFit()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    output = model(x)
    loss = criterion(output, y)

    # Backward pass and optimization
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')

# Plot the results
model.eval()
with torch.no_grad():
    y_pred = model(x)

plt.plot(x.numpy(), y.numpy(), label='exp(x)')
plt.plot(x.numpy(), y_pred.numpy(), label='a*LeakyReLU(x) + b', linestyle='--')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('Fitting a*LeakyReLU(x) + b to exp(x)')
plt.show()

# Print learned parameters
print(f'Learned parameters: a = {model.a.item()}, b = {model.b.item()}, negative slope = {model.negative_slope.item()}')

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Define the model
class LeakyReLUFit(nn.Module):
    def __init__(self):
        super(LeakyReLUFit, self).__init__()
        self.a = nn.Parameter(torch.randn(1))
        self.b = nn.Parameter(torch.randn(1))
        self.c = nn.Parameter(torch.randn(1))
        #self.b = 1.0
        self.negative_slope = nn.Parameter(torch.tensor(0.14867693185806274))  # Initialize with a small positive value

    def forward(self, x):
        return self.a * (torch.relu(self.c * x) - self.negative_slope * torch.relu(-x)) + self.b

# Generate data
x = torch.linspace(-2, 2, 100).unsqueeze(1)  # 100 points from -2 to 2
y = torch.exp(x)

# Initialize the model, loss function, and optimizer
model = LeakyReLUFit()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    output = model(x)
    loss = criterion(output, y)

    # Backward pass and optimization
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')

# Plot the results
model.eval()
with torch.no_grad():
    y_pred = model(x)

plt.plot(x.numpy(), y.numpy(), label='exp(x)')
plt.plot(x.numpy(), y_pred.numpy(), label='a*LeakyReLU(x) + b', linestyle='--')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('Fitting a*LeakyReLU(x) + b to exp(x)')
plt.show()

# Print learned parameters
print(f'Learned parameters: a = {model.a.item()}, b = {model.b.item()}, negative slope = {model.negative_slope.item()}')

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Define the model
class LeakyReLUFit(nn.Module):
    def __init__(self):
        super(LeakyReLUFit, self).__init__()
        self.a = nn.Parameter(torch.randn(1))
        self.b = nn.Parameter(torch.randn(1))
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.14867693185806274)

    def forward(self, x):
        return self.a * self.leaky_relu(x) + self.b

# Generate data
x = torch.linspace(-2, 2, 100).unsqueeze(1)  # 100 points from -2 to 2
y = torch.exp(x)

# Initialize the model, loss function, and optimizer
model = LeakyReLUFit()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    output = model(x)
    loss = criterion(output, y)

    # Backward pass and optimization
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')

# Plot the results
model.eval()
with torch.no_grad():
    y_pred = model(x)

plt.plot(x.numpy(), y.numpy(), label='exp(x)')
plt.plot(x.numpy(), y_pred.numpy(), label='a*LeakyReLU(x) + b', linestyle='--')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('Fitting a*LeakyReLU(x) + b to exp(x)')
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Define the model
class LeakyReLUFit(nn.Module):
    def __init__(self):
        super(LeakyReLUFit, self).__init__()
        self.a = nn.Parameter(torch.randn(1))
        self.b = nn.Parameter(torch.randn(1))
        #self.b = 1.0
        self.negative_slope = nn.Parameter(torch.tensor(0.1))  # Initialize with a small positive value

    def forward(self, x):
        return self.a * (torch.relu(x) - self.negative_slope * torch.relu(-x)) + self.b

# Generate data
x = torch.linspace(0, 2, 100).unsqueeze(1)  # 100 points from -2 to 2
y = torch.exp(x)

# Initialize the model, loss function, and optimizer
model = LeakyReLUFit()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    output = model(x)
    loss = criterion(output, y)

    # Backward pass and optimization
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')

# Plot the results
model.eval()
with torch.no_grad():
    y_pred = model(x)

plt.plot(x.numpy(), y.numpy(), label='exp(x)')
plt.plot(x.numpy(), y_pred.numpy(), label='a*LeakyReLU(x) + b', linestyle='--')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('Fitting a*LeakyReLU(x) + b to exp(x)')
plt.show()

# Print learned parameters
print(f'Learned parameters: a = {model.a.item()}, b = {model.b.item()}, negative slope = {model.negative_slope.item()}')

