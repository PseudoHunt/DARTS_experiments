# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BqVsEG_V5upizFld1wSqtSvYsxcvXIkz
"""

!pip install tensorly==0.5.1

from torchvision import models
model = models.vgg19(pretrained=True).eval()

layer = model.features._modules['0']
print(layer)

import tensorly as tl
from tensorly.decomposition import parafac
import torch
import torch.nn as nn

tl.set_backend('pytorch')

from tensorly.decomposition import partial_tucker

def tucker_decomposition_conv_layer(layer, ranks):
    """ Gets a conv layer,
        returns a nn.Sequential object with the Tucker decomposition.
    """
    core, [last, first] = \
        partial_tucker(layer.weight.data, \
            modes=[0, 1], rank=ranks, init='svd')

    # A pointwise convolution that reduces the channels from S to R3
    first_layer = torch.nn.Conv2d(in_channels=first.shape[0], \
            out_channels=first.shape[1], kernel_size=1,
            stride=1, padding=0, dilation=layer.dilation, bias=False)

    # A regular 2D convolution layer with R3 input channels
    # and R3 output channels
    core_layer = torch.nn.Conv2d(in_channels=core.shape[1], \
            out_channels=core.shape[0], kernel_size=layer.kernel_size,
            stride=layer.stride, padding=layer.padding, dilation=layer.dilation,
            bias=False)

    # A pointwise convolution that increases the channels from R4 to T
    last_layer = torch.nn.Conv2d(in_channels=last.shape[1], \
                                 out_channels=last.shape[0], kernel_size=1, stride=1,
                                 padding=0, dilation=layer.dilation, bias=True)

    last_layer.bias.data = layer.bias.data

    print('first',first)
    first_layer.weight.data = \
        torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)
    last_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)
    core_layer.weight.data = core


    new_layers = [first_layer, core_layer, last_layer]
    return nn.Sequential(*new_layers),core,[last,first]

print('* Before the decomposition:')
print(layer)
layer_tucker_decomposed,core,factors = tucker_decomposition_conv_layer(layer, ranks=[16, 16])
print('\n* After the decomposition:')
print(layer_tucker_decomposed)

import numpy as np

# Import PyTorch
import torch
from torch.autograd import Variable

# Import TensorLy
import tensorly as tl
from tensorly.tucker_tensor import tucker_to_tensor
tl.set_backend('pytorch')

from tensorly.random import check_random_state

rec = tucker_to_tensor([core, factors])

loss = tl.norm(rec - layer.weight.data, 2)

loss

import torch.nn.functional as F

# Convert rec to a Tensor
rec_tensor = torch.from_numpy(rec.numpy()).float()

# Calculate the loss
loss = F.mse_loss(rec_tensor, layer.weight.data)

# Print the loss
print(loss)

n_iter = 10
lr = 0.0005
penalty = 0.1
tensor = layer.weight.data
core.requires_grag = True
factors[0].requires_grad = True
factors[1].requires_grad = True
optimizer = torch.optim.Adam([core]+factors, lr=lr)

for i in range(1, n_iter):
    # Important: do not forget to reset the gradients
    optimizer.zero_grad()

    # Reconstruct the tensor from the decomposed form
    rec = tucker_to_tensor([core, factors])


    # squared l2 loss
    loss = tl.norm(rec - tensor, 2)
    print(loss,'loss')

    # squared l2 penalty on the factors of the decomposition
    for f in factors:
        loss = loss + penalty * f.pow(2).sum()

    loss.backward()
    optimizer.step()

    if i % 1 == 0:
        rec_error = tl.norm(rec.data - tensor.data, 2)/tl.norm(tensor.data, 2)
        print("Epoch {},. Rec. error: {}".format(i, rec_error))

random_state = 1234
rng = check_random_state(random_state)

core_rnd = tl.tensor(rng.random_sample(core.shape),requires_grad=True)

first_rnd = tl.tensor(rng.random_sample(factors[1].shape),requires_grad=True)

last_rnd = tl.tensor(rng.random_sample(factors[0].shape),requires_grad=True)

factors_rnd = [last_rnd,first_rnd]

n_iter = 20000
lr = 0.000005
penalty = 0.0
tensor = layer.weight.data
#core.requires_grag = True
#factors[0].requires_grad = True
#factors[1].requires_grad = True
optimizer = torch.optim.Adam([core_rnd]+factors_rnd, lr=lr)

for i in range(1, n_iter):
    # Important: do not forget to reset the gradients
    optimizer.zero_grad()

    # Reconstruct the tensor from the decomposed form
    rec = tucker_to_tensor([core_rnd, factors_rnd])


    # squared l2 loss
    loss = tl.norm(rec - tensor, 2)
    if i % 100 ==0:
      print(loss,'loss')

    # squared l2 penalty on the factors of the decomposition
    for f in factors:
        loss = loss + penalty * f.pow(2).sum()

    loss.backward()
    optimizer.step()

    if i % 100 == 0:
        rec_error = tl.norm(rec.data - tensor.data, 2)/tl.norm(tensor.data, 2)
        print("Epoch {},. Rec. error: {}".format(i, rec_error))
        print(loss)

